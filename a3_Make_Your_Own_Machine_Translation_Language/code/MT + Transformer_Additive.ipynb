{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchdata\n",
    "# !pip3 install torchtext==0.16.2\n",
    "# !pip3 install torch==2.2.0\n",
    "# !pip3 install datasets\n",
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !pip3 install pythainlp\n",
    "# !pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'input_text' #en\n",
    "TRG_LANGUAGE = 'translated_text' #th\n",
    "\n",
    "dataset = datasets.load_dataset(\"kvush/english_thai_texts\", split={'train': 'train[:70%]', 'validation': 'train[70%:90%]', 'test': 'train[90%:100%]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "type(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row['input_text'], row['translated_text']) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 41901 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [(row['input_text'], row['translated_text']) for row in dataset['validation']]\n",
    "test = [(row['input_text'], row['translated_text']) for row in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11972"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize\n",
    "\n",
    "def thtokenizer(sentence):\n",
    "    # Tokenize the sentence using PyThaiNLP's word_tokenize function\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = thtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Service, scallops, all - top notch in every way!\n",
      "Tokenization:  ['Service', ',', 'scallops', ',', 'all', '-', 'top', 'notch', 'in', 'every', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!\n",
      "Tokenization:  ['บริการ', 'หอย', 'เชลล์', 'ทั้งหมด', ' ', '-', ' ', 'โดดเด่น', 'ใน', 'ทุก', 'ด้าน', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", sample[1])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 12, 10, 0, 10]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haircut'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16607"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 43])\n",
      "Thai shape:  torch.Size([64, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim  = hid_dim            # Hidden dimension\n",
    "        self.n_heads  = n_heads            # Number of attention heads\n",
    "        self.head_dim = hid_dim // n_heads # Dimension of each attention head\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        # Implement additive attention\n",
    "        self.W1 = nn.Linear(self.head_dim, self.head_dim)  # Transformation for decoder\n",
    "        self.W2 = nn.Linear(self.head_dim, self.head_dim)  # Transformation for encoder\n",
    "        self.vt = nn.Linear(self.head_dim, 1)  # Weight vector for computing attention scores\n",
    "\n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Project Q(Query), K(key), and V(Value) to hidden dimension\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        # Reshape each of them for Multi-Head Attention\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q,K,V = [batch_size, n heads, src len, head_dim]\n",
    "\n",
    "    #Additive Attention\n",
    "\n",
    "        # Lenght of query and key\n",
    "        q_len = query.shape[1]\n",
    "        k_len = key.shape[1]\n",
    "\n",
    "        # Apply the additive attention mechanism\n",
    "        qw1 = self.W1(Q).view(batch_size, self.n_heads, q_len, 1, self.head_dim) # [batch_size, n head, query len,1, head_dim]\n",
    "        kw2 = self.W2(K).view(batch_size, self.n_heads, 1, k_len, self.head_dim) # [batch_size, n head, 1, key len, head_dim]\n",
    "\n",
    "        # Compute energy by using tanh activation\n",
    "        energy = torch.tanh(qw1 + kw2)\n",
    "\n",
    "        # Find the attention scores by applying weight (v) that is transposed on energy term\n",
    "        # energy = self.vt(energy).view(batch_size, self.n_heads, q_len, k_len @ self.n_heads, 1)\n",
    "        energy = self.vt(energy).squeeze(-1) # [batch_size, n_head, q_len, k_len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "# OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "# HID_DIM = 256\n",
    "# ENC_LAYERS = 3\n",
    "# DEC_LAYERS = 3\n",
    "# ENC_HEADS = 8\n",
    "# DEC_HEADS = 8\n",
    "# ENC_PF_DIM = 512\n",
    "# DEC_PF_DIM = 512\n",
    "# ENC_DROPOUT = 0.1\n",
    "# DEC_DROPOUT = 0.1\n",
    "\n",
    "# enc = Encoder(INPUT_DIM, \n",
    "#               HID_DIM, \n",
    "#               ENC_LAYERS, \n",
    "#               ENC_HEADS, \n",
    "#               ENC_PF_DIM, \n",
    "#               ENC_DROPOUT, \n",
    "#               device)\n",
    "\n",
    "# dec = Decoder(OUTPUT_DIM, \n",
    "#               HID_DIM, \n",
    "#               DEC_LAYERS, \n",
    "#               DEC_HEADS, \n",
    "#               DEC_PF_DIM, \n",
    "#               DEC_DROPOUT, \n",
    "#               device)\n",
    "\n",
    "# SRC_PAD_IDX = PAD_IDX\n",
    "# TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16607, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (vt): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11664, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (vt): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (vt): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11664, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251392\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 11664\n",
      "______\n",
      "14259193\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 6s\n",
      "\tTrain Loss: 4.950 | Train PPL: 141.174\n",
      "\t Val. Loss: 4.001 |  Val. PPL:  54.675\n",
      "Epoch: 02 | Time: 1m 5s\n",
      "\tTrain Loss: 3.732 | Train PPL:  41.746\n",
      "\t Val. Loss: 3.514 |  Val. PPL:  33.597\n",
      "Epoch: 03 | Time: 1m 7s\n",
      "\tTrain Loss: 3.215 | Train PPL:  24.905\n",
      "\t Val. Loss: 3.206 |  Val. PPL:  24.672\n",
      "Epoch: 04 | Time: 1m 7s\n",
      "\tTrain Loss: 2.641 | Train PPL:  14.033\n",
      "\t Val. Loss: 2.698 |  Val. PPL:  14.852\n",
      "Epoch: 05 | Time: 1m 5s\n",
      "\tTrain Loss: 2.123 | Train PPL:   8.358\n",
      "\t Val. Loss: 2.398 |  Val. PPL:  10.999\n",
      "Epoch: 06 | Time: 1m 5s\n",
      "\tTrain Loss: 1.785 | Train PPL:   5.960\n",
      "\t Val. Loss: 2.284 |  Val. PPL:   9.819\n",
      "Epoch: 07 | Time: 1m 4s\n",
      "\tTrain Loss: 1.554 | Train PPL:   4.731\n",
      "\t Val. Loss: 2.184 |  Val. PPL:   8.885\n",
      "Epoch: 08 | Time: 1m 4s\n",
      "\tTrain Loss: 1.375 | Train PPL:   3.955\n",
      "\t Val. Loss: 2.146 |  Val. PPL:   8.549\n",
      "Epoch: 09 | Time: 1m 4s\n",
      "\tTrain Loss: 1.236 | Train PPL:   3.443\n",
      "\t Val. Loss: 2.139 |  Val. PPL:   8.490\n",
      "Epoch: 10 | Time: 1m 5s\n",
      "\tTrain Loss: 1.122 | Train PPL:   3.071\n",
      "\t Val. Loss: 2.146 |  Val. PPL:   8.546\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}_additive.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/Seq2SeqTransformer_additive.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.118 | Test PPL:   8.311 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service, scallops, all - top notch in every way!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  153,    6, 2910,    6,   64,   58,  508, 1441,   19,  250,  219,\n",
       "          11,    3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   28, 1352, 2288,  168,    4,  150,    4,  661,   11,  326,  422,\n",
       "          13,    3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 11664])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 11664])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11664])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1352, 2288,    4,    4,  150,    4,  164,   12,  326,  422,   13,    3,\n",
       "          13])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หอย\n",
      "เชลล์\n",
      " \n",
      " \n",
      "-\n",
      " \n",
      "สุดยอด\n",
      "มาก\n",
      "ทุก\n",
      "ด้าน\n",
      "!\n",
      "<eos>\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 14, 14])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Service',\n",
       " ',',\n",
       " 'scallops',\n",
       " ',',\n",
       " 'all',\n",
       " '-',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'in',\n",
       " 'every',\n",
       " 'way',\n",
       " '!',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'หอย',\n",
       " 'เชลล์',\n",
       " ' ',\n",
       " ' ',\n",
       " '-',\n",
       " ' ',\n",
       " 'สุดยอด',\n",
       " 'มาก',\n",
       " 'ทุก',\n",
       " 'ด้าน',\n",
       " '!',\n",
       " '<eos>',\n",
       " '!']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/_hhynlm52_l1mrgsb4gssb1r0000gn/T/ipykernel_42446/3489918703.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "/var/folders/h9/_hhynlm52_l1mrgsb4gssb1r0000gn/T/ipykernel_42446/3489918703.py:19: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3618 (\\N{THAI CHARACTER YO YAK}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3648 (\\N{THAI CHARACTER SARA E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3594 (\\N{THAI CHARACTER CHO CHANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3660 (\\N{THAI CHARACTER THANTHAKHAT}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3626 (\\N{THAI CHARACTER SO SUA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3640 (\\N{THAI CHARACTER SARA U}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3617 (\\N{THAI CHARACTER MO MA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAANVCAYAAACZFHS8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABbi0lEQVR4nO3debxUdf0/8PcAcgHhXnFHuQmaIqKpCIlYgrmhWS65YKbiri2ukIBl5IZmGqZlliam5q5pKLnE4hIGLpSSuyIoLslyLwjeC9zP7w9+zNcbWB5lPHMvz+fjMQ+9c2bmvg4zd+a85vOZzxRSSikAAAD4RFrkHQAAAKApUaIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKKAVSKl9F9/BgBoLpQo4DNraGiIQqEQEREzZsyIlFLxZwCA5kaJAj6ThoaGaNFi2VPJeeedF2effXZMnDgx51QAAKWjRAGfyfICddZZZ8Uvf/nLOPDAA6N79+6NLtPQ0JBHNACAkmiVdwD4b0wLaxrGjh0bt9xySzz88MOx7bbbxtKlS+O9996LF154IXbaaadYY401Go1YAQA0ZUoUZWN5YZo9e3YsXbo0OnbsGGussUbesfgEli5dGmuttVZsvPHG8cILL8Qf//jH+MMf/hANDQ2x7rrrxhNPPBGtW7fOOyYAwCrhbWHKwvICde+998aAAQOiX79+0bNnz/jFL34Rb775Zt7x+IiVTc1r1apVNDQ0xCGHHBL9+/ePN998M4YOHRqjR4+O9957L8aPH59DUgCA0jASRVkoFArx4IMPxre//e0YMWJEfOc734kRI0bET37yk+jWrVt07tw574hE40UkZs6cGYsWLYotttgiBgwYECmlmDJlSnzve9+Lfv36xbrrrhuzZs2K9dZbL9Zcc82ckwMArDqF5MtcyFlKKZYuXRqDBg2KDTfcMH7+85/He++9F3379o099tgjrrrqqoiIWLx4sel9ZWLYsGFx2223xb///e/o3r17HH300XHkkUdGu3btIiJiyZIlUVNTE4MGDYp58+bFhAkTomXLljmnBgBYNUznI3eFQiFatWoVc+fOjT333DPmzZsX22+/fey2227FAnXXXXfFs88+m3PS1ddHp/CNHj06rr/++rjgggtizJgxsfnmm8fo0aPj3HPPjQ8++CBSSnHFFVfEd77znXjnnXdi3Lhx0bJly1i6dGmOewAArA5WNj5UilWClagmprkMHC7fj3nz5hXPa9euXVx00UXRs2fP2H///ePKK6+MiIiFCxfGH//4x5gwYYKlsnOyfArf/fffH3PmzIkf/ehHMXDgwNhll13iuuuuiz333DMefPDBmDBhQhQKhejUqVP069cvJk2aFGussUYsWbLESBQAUDLLjy2Xr+o8b968mDp1akRESVYHNp2vifjPpb7fe++9ePfdd6Nt27bxxS9+Mcdk2S3fl7Fjx8a1114bxx13XAwYMCD+/ve/x7HHHhv19fXx0ksvFS9/9tlnx8033xwPPfRQbLbZZjkmX729/fbbUV1dHQ0NDfHDH/4wLrrookaPyz59+kTXrl3j5ptvbnS9pUuXKlAAQMl89HhkyZIlce2118aYMWPivvvui1/96ldx8sknr/LfaSSqCWhoaCg+MOrq6uKqq66KI444InbdddcmuepZoVCIP/3pT3HggQdG7969o2PHjhERsfXWW8dJJ50Uixcvjr59+8aJJ54YBx98cPzmN7+JO++8U4H6nP3n+yudOnWKyZMnR9euXeORRx6Jt956q9H2/v37x4IFC2Lx4sWNzlegAJqnd999N+8IEBHLji0XLlwYP/nJT2LfffeNESNGxPrrrx/V1dWx/fbbl+R3KlFNQIsWLeLDDz+MYcOGxYEHHhg//elPo1OnTrHGGmtEt27d8o73iXz0gPzNN9+MH//4x3HhhRfGWWedFTvuuGNERKy55poxaNCguOmmm6Jr164xb9682HTTTeNvf/tbyf4AWLmPFvelS5cW77+ePXvGzTffHC+99FJ897vfjZdffjk+/PDDWLRoUYwfPz7WWWcdi38ArAb++Mc/xhlnnBH19fWm2pOrJ598MkaOHBk9evSIhx9+OHbZZZd44403okWLFtG1a9ficeaqZonzMvf444/Ho48+Gr/5zW+iU6dO8a1vfSvuvPPOGDJkSGy55Zaxyy675B3xv7r00ktj9913j2233bZ43uzZs6OmpqZR9uXDsO3bt4++fftG375984hLNF7GfNSoUfH000/Ha6+9FgMHDox+/frFl7/85bjvvvti3333jd133z26desWlZWVUV9fH7/97W8jYsXppwAR//fcsHTp0mjRokUUCoVGzzkfvQzl7a233oqHHnooPvjgg+jYsaP7jVz86U9/ih/84AfRu3fvOP7442PYsGERETF16tSYPHlyXHPNNSt9nlkVjESVqZRS/O1vf4uvfvWrMXXq1DjppJNi0qRJMXjw4HjppZfi8ccfj/POOy8iomxXPXv55Zdj8uTJ0bZt20bnL3/H6qOLSiw3bty4+Mtf/lL82Uf2Pj/L/62XP8kMHTo0zjvvvKiuro4NN9wwRo8eHaecckpMmjQpdtxxxxg7dmy0bt06Xn755TjzzDPjqaeeitatW8fixYu9kAIr9dxzz0XEsqk3y78f8Kijjorvf//7cfvttxe3ee4vX8uPOYYMGRKbbrppnHPOORERnvfJRd++feOWW26J6667LoYPH97ouWX99dcvfs9oKRaWUKLKVKFQiL59+8bkyZPj97//fQwdOrS47b777osOHTrEpptuGhHl+5mTzTffPK699trYYost4m9/+1s8/fTTERHRpUuXqKioiF/96lcxZ86ciPi/J9/77rsvbr311li0aFGj8ym95e/URCx7B+fuu++Ou+66Ky644IK444474vzzz4911lknzjvvvJgxY0b06tUrbr311pg/f35cdtllsWjRomhoaDCdD1ip8ePHx7bbbhu///3vo0WLFvGXv/wlvvGNb8SiRYti2rRpccIJJ8TFF18cEYpUOVt+zLFkyZLYd99947nnnosFCxZEhDc++fxMnz493n777Vh//fVj5513jqqqquK2adOmxYUXXhhHHnlkdOrUqXQhEmXn9ddfT++9995Ktz3//PNpnXXWSTfccMPnnCqbpUuXFv+/pqYmDRgwIG255ZbpySefTCmlNGnSpNS+ffv0jW98I911111p/Pjx6bTTTkuVlZXpueeeyyv2aun73/9+uvDCCxudN3ny5LTWWmulv//9743Ov+uuu1KXLl0anT958uS04YYbpj322CPV1NR8LpmBpueNN95IZ555ZurYsWO67rrr0o033ph+9atfpZRSeuedd9Jll12WCoVCGjlyZPE6DQ0NecXlP1x//fVp2223TQ899FB68803U0opvfnmm6ljx47p4osvzjkdq5O777479enTJ/3yl79MCxYsKJ6//NjzkksuSQcccECaP39+SXMYiSoz99xzT+yzzz7x4IMPNprulv7/uzsPPvhgfPWrX4199tknp4SfzPJh05deeinWXHPN+OEPfxhbbrllnHTSSfHkk09Gnz594m9/+1u8+eabMWTIkDj22GPjsccei4kTJ0aPHj1yTr/6mDVrVnz44Yfxhz/8ofi9XBERrVu3jvXWWy9mzJgREf/3+DvggAMipRQTJ04sXrZ3795x1113xauvvhq1tbWf7w4ATcYXvvCFOP300+P444+P0047rbh6VkTEBhtsEMcdd1xceumlMXz48PjZz34WEWYjlIvzzjsvKioqokuXLnHWWWfF17/+9bj++uujdevWccEFF8Rjjz0WM2fOzDsmq4F77rknDjvssBg4cGAceOCBseaaaxa3tWjRIpYuXRq33HJLbLnlltG+ffvShilpRSOTe+65J6255prp0ksvTTNmzFhh+8KFC1N1dXU688wzc0iX3auvvpp22GGHNG7cuJRSSn/5y1/SN77xjdSrV680efLklNKyUarp06enl19+Oc2ZMyfPuKudxYsXp5RSeuGFF9LgwYNTt27d0i9/+cvi9gMOOCBVV1enp556qnje+++/n7bddtv0xz/+cYXbW7RoUelDA03SR2cnvP322+nss89OFRUV6ZJLLml0udra2jRq1KhUKBTSL37xi885JSvzu9/9LhUKheJrweOPP55GjBiRNt5447TXXnulbbfdNm222Wbp0UcfTSk1/dHDpp6/OXv77bdT7969i8cqH374YXr//ffT7bffnp5++umUUkqzZ89OQ4YMSXV1dSml0t6fSlSZmD17dtpxxx3T+eefn1Ja9sCYM2dOuu2229IjjzxSvNxHhy7L/Q994cKFaZtttknf+c53iuc9+OCDxSK1fGofn78zzjgjbbbZZsUnmRdeeCGdeeaZqVu3bumyyy4rXq5fv35po402SkOHDk2/+MUv0h577JG+9KUvFQsYwP+y/LXqySefTOPHj091dXXp7bffToMHD06tW7dO1113XaPL19TUpF//+tfpX//6Vw5p+agHHnggnXvuuen2229fYdtzzz2X/vCHP6S+ffumQqGQdtlll1RbW5tDylXjP6evU35qa2vTdtttl6666qq0aNGi9KMf/SjtvPPOacMNN0ytWrVKY8aMSSn935vEpT5ONp2vTKT/P11qk002iRkzZsT5558fBx54YAwaNChOP/30+OUvfxkRESeddFJx6LIcpzksX5hg8eLF0bZt2/jlL38Zjz76aIwdOzYiIvbYY4849dRTo7q6Og499ND45z//mWfc1VJDQ0PsuuuuUVlZGbvuumvU19dHt27d4vjjj4999903rr766hg1alREREyYMCEOOuigePLJJ+Pmm2+O9dZbL5588slo1apV2a4KCZSP9P+Xvb7zzjtjwIAB8fjjj8eMGTNiww03jNNOOy1OO+20OOWUU2L06NHF61RWVsZJJ50U3bt3zy84MWnSpDjxxBPj5z//ebRr1y4ili0msfx4pUePHnHEEUfExIkT48orr4xFixYVp4A3NWPHjo2TTz55hS+Rp7zU19fHtttuG1dffXWst9568eyzz8bAgQNj6tSpsccee8Qdd9wRKaVo1WrZNziV+jjZ90SViXXWWSeqqqrinHPOiX//+9+x5557xqGHHho33HBDHHPMMfHaa69FRJT9ymezZs2Kzp07F3N27do1Nt9885g0aVLsvffeERGx2267RX19fdxwww3RoUOHPOOullq0aBF77713tGnTJgYPHhz9+vWLiRMnFotURMRVV10VKaU4/fTT4/LLL48PPvggIiLatWsXhUIhlixZUnySAvg4hUIhHnvssTjmmGPi5z//eXz7298uvhG48cYbxw9+8IOIiDjzzDOjrq4uTjzxxOL1yFeXLl3iuOOOi1/84hdx8803xz777FN8A235Cn1Lly6NVq1axUknnRSjRo2Km266KS688MKck2dXXV0dL7zwQjz88MNx1FFH5R2Hj5g5c2bMmzcvNthgg1h//fXj4osvjieeeCLmzJkThx56aLHgt23bNqqrqz/f546SjnPxX73yyitp2rRp6Yknniied/PNN6ebb745ffjhh2nJkiUppZS+/e1vp9NOOy0tXbq0LKfwLc/06quvps6dO6fDDjssjRs3rpj/d7/7XWrdunWaNm1ao+t98MEHn3tW/u/+WrJkSXrooYfStttum/r06bPC1L4tt9wyXX755R97fYD/ZvnnoIYPH57222+/RtuWvz6klNJ7772XTj755FRdXZ3mzZvnOaYMfPjhhymllObNm5cuuuii1KVLl0afx/7o/bf8/w899NA0ZMiQRp9/K2fLj6mW5x8xYkTaaaediisPkr8777wzde3aNX3hC19I66yzTvr2t79d/Ez9cv/+97/T8OHD07rrrpuef/75zzWfEpWTO+64I3Xp0iV17dq1uNT3fy7tPXfu3DR8+PDUsWPHz/2BkdX111+f9t9///Twww+nHXfcMfXp0yf169cvPf300+mtt95KRx55ZDr11FPThx9+2GSeYJublR2Y1NfXpwceeGClRWrIkCFprbXWWulceGDVaurF4aPP6/X19SmlVHw+OeSQQ9K3vvWtFS6XUkrPPvtsqq+vT++991569913P6e0fJxRo0alY445JvXs2TNdc801afr06WnhwoVp5MiRqUePHmnIkCHFy370vhw3blxq2bJlevbZZ/OI/an8Z1m6//770+abb54mTZqUUlrxscrn69FHH03t2rVLo0aNSv/617/SNddck/bZZ5+08847F++jO++8Mw0aNChtsskmxYUlPk9KVA4ee+yx1L59+3TNNdekJ598Mj3xxBNps802S/37909Tp05NKS1bA/9rX/ta2myzzXJ5YHwSy1/0Z86cmbp06ZIuuuiilNKyDwUvX4lvk002SQcddFDq1atX2mmnndLcuXNzTLz6+uiLwUsvvZRmzJiRZs6cmVJadqDz4IMPpm233TbtuOOOxXcgn3vuuXTllVc2escRWPU++vc5a9as9MEHHxRH6ptSuZo5c2aaPXt2SimlP//5z+kPf/hDSiml888/P3Xs2DFNnz49pfR/+ztnzpw0dOjQ9Pjjj+cTmEbOOuustMEGG6QLLrggnX/++amqqiodeeSRqa6uLr333ntp5MiRaeutt04nnHDCSq/flEZw7rrrrlQoFNKQIUOKixGktKzw9+nTx+tejpY/551zzjnpm9/8ZqNt48aNS3vttVc67rjjUkrL3oT57W9/m1577bXPPWdKSlQufvazn6X+/fs3mp73zjvvpC5duqSBAwemlJYNj//mN79Jr776ap5R/6e//e1v6ayzzkonnHBCWrx48Qqrtt16661p2LBhqVAopEKhUDxw5/Pz0YOwn/70p+lLX/pS+uIXv5i23nrrNHbs2JTSsneOH3zwwbTddtulvn37rrBcuRcUKI2P/n3++Mc/Tj179kxf/OIX04EHHlj8eoimUKSWf6n67rvvnn7/+9+nQqGQbr311pRSSi+//HLq169f6tOnT3r99ddTSstWzzr77LPTF77whZV+pQefr8cffzx98YtfLE6VmjJlSioUCumGG24oXmbOnDlp+PDh6fDDD2/0mGyKrw/Tp09PN9xwQ9p1111T9+7d05577pnGjRuX7rrrrrTffvsV//aMRuXnxz/+cerVq1ejL9NNKaXLL788rbfeesWvxcnzPlKicnD66aen3r17F39efsA6bty4tNZaazWZ4fDa2tp03HHHpcrKytS/f//i+UuWLFnhQf2vf/2r7Athc3fOOeek9ddfP40ZMyY988wz6Zvf/GZq2bJlcbpefX19euihh9JGG22Ujj/++JRS0zh4Y5kpU6bkHYGMPvo8ec0116S11147XX/99en8889Phx12WGrdunX685//nFIq/7/FJUuWpLvvvjttscUWaY011ki/+tWvUkr/l/u+++5Le+21V+rQoUPabbfd0le/+tW07rrrlu1Mi9XNhAkT0k477ZRSWvbZ7Pbt26df//rXKaVlr/UTJ05MKaVGn1kr98fkyvxn4Zs1a1aaOnVqGjBgQOrfv3/q3LlzKhQKafDgwTklZLnrrrsurbfeemn8+PGNHmuTJk1KW2yxRW6jTx+lRH1Opk+fnt5///2UUkrjx49PFRUVafTo0Y0uM27cuPTFL34xvfHGG3lE/FQmT56cjj322NSyZctGX8D60Qe8d3Ly98QTT6Sdd945jR8/PqW0bKrNWmutlfr165datGiR7rjjjpTSsql9kydPbpLvLK7OJk2alAqFQho1alTeUfgUHn/88XTsscema6+9tnjeO++8k0499dRUWVlZ9t+pt/z5/qWXXkqdO3dOXbp0Sfvtt1/xNW+5t99+O/3qV79KZ5xxRrrooovSyy+/nEdcPmL+/PkppZTuvffetOmmm6Zbb701VVVVFUtwSssK8MCBAxsdtDbFAnXVVVelE088MR122GHpzjvvLO77ck899VS65JJL0mabbZY6deqUHnvssZySrp6effbZNHHixOIIdkopHXTQQWmjjTZKDz/8cHGq8Omnn5623nrrsvh4iBL1OfjTn/6U+vbtm371q1+lBQsWpHnz5qXBgwenTTfdtPglg8u/NGzrrbdO//73v/MN/DGWP2nOmzevUcbXXnstHXnkkWmLLbZotAhBU3ySbS7+s7i++OKL6YILLkgNDQ3poYceShtuuGH69a9/nebMmZN23HHH1Lp163T99dc3uo4i1XQsWrQojRw5Mq2xxhrFb3KnaRg/fnzabLPN0jrrrLPC3+D06dPTLrvskkaOHJlSKv/n1NmzZ6dp06alO+64I+20005pn332KRYpb6aVn9/+9repR48exZ8HDBiQCoVC+tnPflY8b9GiRWnfffdNhxxySJO+D88666y07rrrph/+8Idpv/32S717905DhgxZ6ZcDP/XUU6lv377pN7/5TUqp/P/umoM77rgjVVdXpy9/+cupU6dOqWfPnumBBx5IDQ0Nab/99kudOnVKW2yxRerfv3/q2LFj2YxgK1El9qc//Sm1adMmjRo1qtG87zfeeCOdeeaZaY011kjdu3dPvXr1Suuss07ZPDD+0/InkXvvvTf17ds3bbnllql3797p6quvTgsWLEjPP/98OvbYY1P37t3TnXfemXPa1dtHX+gmT55cXCFr3rx5KaWUDjvssHTKKacUL/ed73wndevWLX3lK1/5/MPymVx33XXFkesPP/wwXXzxxalQKChSTcy5556b1llnnTRgwIAVPpy/5557pkGDBuWU7L9b/rrwxhtvpOnTp6dXXnklpbTsOeiWW25Jffr0Sfvuu2/xHeTLL7883XjjjWnJkiUOTMvAk08+mXr06JHuvvvulNKy1em+8pWvpC996Uvp3nvvTb/73e/SXnvtlXr06FH8vHNTLFLXXntt2nTTTdNTTz2VUlp2HNOiRYvUo0ePdMoppxQ/c7N8VcmUUjrjjDMaLbRE6UyaNCmtvfbaxdlZL7/8cioUCo1GQ++44470i1/8Iv3iF78oPs+UAyWqhGbNmpV69uyZrrjiipTSsoOc999/P919993FaQyTJk1KF154Yfrd735XVg+MlRk7dmxq165dGjlyZHrttdfSYYcdltZaa6300EMPpZRSmjp1ajrhhBPSBhtskP70pz/lnHb19NEDk7PPPjv16tUr/eY3vymeX1NTk7p3715cSfGDDz5IBx54YHr44Ycd1DQxtbW1aYMNNkjbb799ccGWRYsWKVJl7L8dgJ577rmpR48e6cwzzyyO9C9atCh9+ctfbrSsdLlY/nxx5513pi222CJ17do1VVVVpZNPPrlY7G+55Zb0la98JW211VbpxBNPTIVCYYWv8iA/77//fvra175WXOlsyZIl6a9//Ws66KCD0gYbbJB23nnndMQRRxTLRVOcnTB//vx02223pXPOOSeltGzl444dO6bLL788DRkyJK299tpp8ODBqaamJqX0f4/r7373u2nAgAErLLLEqnf11VenAw44IKW07OtVNt100+JjsqGhYYUFy8qJElUiDQ0Nae7cuWmbbbZJv//971NdXV0655xz0s4775zWW2+9VFFRkf7617/mHXOllr/Qf/QFf9GiRemggw5Kw4YNSyktm7bRtWvXdPLJJze67jPPPJN+8IMflH0hbO5+8pOfpHXWWSdNmDAhvf322422nXrqqaldu3Zp2LBhqU+fPmmHHXYovjg2xXcZV2czZsxIW2+9derdu7ciVeY++rd17bXXppNOOimdeuqp6be//W3x/HPOOSdtuummqUePHumYY45JBx54YNpqq60avUNeTiZMmJDatm2brrrqqjR+/Ph01113pXXXXTcdcMAB6c0330xLly5NDzzwQDrhhBPSN7/5zSazaFJztnxGwnIPPvhgat26dbr//vsbnT9r1qy0ePHiYqko5wPZj/PHP/4xnXTSSentt99O7777bnrrrbfStttum37+85+nlJYtyd6pU6dUXV2dLrnkkpTSsr/Tt956K33xi18s+88iNnXLC+oZZ5yRvv3tb6clS5akzp07pxNOOKH4uLvxxhvTL37xi7JdzESJKoHRo0enUaNGpblz56bDDz889ezZM1VWVqb99tsvjRo1Ks2aNavRuz/lZPkL/euvv56uvvrqRit+7bHHHumRRx5J77//furUqVOj74q46667iu8wGv7O18yZM9OOO+6YbrvttkbnL79v33jjjXT66aenfv36pe985ztN+l1Glt3fy6fXLi9SpvaVryFDhqT11lsvHXbYYWmfffZJa6yxRjryyCOL288///y09tprp6997WvF1dFSKs+D2OHDh6d99tmn0XnPPPNMWnvttdNpp53W6Pzm8rqw/CBu9uzZZfv55Y/z85//PA0YMCBdeumljc4fOHBg+v73v58WLly40ml75Xbg+kmNGDEi9ezZM7344osppWWLd3Xt2jVNmzYtpZTS008/nQ4++OD029/+doU3EP9zWW1WrdGjR6fLL788pbRsYZ3NNtssrbnmmul73/teo8t973vfS4cddljZ3h9K1Co2a9astM0226QLLrggpbRstZE77rgjXXPNNY1Wgtl///3TT3/607xirtTyJ5F//vOfaYsttkgHHHBAuu+++4rbBwwYkPbbb7+02WabpZNPPrl48F1TU5MOOOCAdOWVVzbZJ9um7D+f/F944YVUWVnZ6AsEl6uvry+WpeXTF1IqzwM0PrmZM2embt26pV69eq1QpFq3bp0uvvjinBOuvj769/nYY4+lDTfcsLhc9OLFi9ODDz6Y1lprrUZvSo0YMSLtvPPOafjw4cWRg3J7bm1oaEhHH3102nPPPVNKy/Zz+ecvb7jhhrT++uunGTNmFPe/3PJ/FnfddVfq06dP2mSTTdLgwYPL9rPM/+mxxx5LJ510UurevXvafvvt09VXX53mzp2bbr/99rT++usXP4/X1O+r5Z/BSyml3r17p9133z2ltOwzYFtuuWW66KKL0vPPP5/23XffNGjQoOL+fvSNxKb+b1DOlh8nX3jhhcWfTz755LTpppsWF9d555130vDhw9N6662X/vWvf+UZ979SolaR5S8U48aNS717905/+9vfVnq5999/v/jAeOGFFz7PiJ/I888/nzp27JiGDh2a3nrrrUbbxo0blzbffPPUrVu3RuefffbZabPNNvM9UDn46AHaXXfdlaZPn148oP7d7363whD4/fffny644AIvFk3Y8vvrhRdeSFOmTEmPPPJISmlZkerRo8cKReonP/lJWnvttYtfTMjn56N/W4sXL05//vOfU9euXVd4V/WOO+5IHTt2LJarlJZN7evVq1c65ZRT0nvvvfe5Zf44Hx2B+eCDD1JKy55zKioqip+LXf58dPfdd6fu3bs3Ophtyj56P06ZMiWtt9566cc//nG64IIL0iabbJIOOOCA4pezlqPrrrsuDR48OP34xz9Of/zjH9OsWbPSD37wg7TTTjulLl26pFtvvTVtsMEG6aijjmryMxIuuOCCtM8++xS/X+35559PW265ZRo1alRasmRJ+u53v1tcwvzLX/5y8c1gr4Ol95/HyZMmTSpue+qpp9KRRx6ZOnbsmDbddNPUq1ev1KVLl7J/g0KJWsV23HHH9J3vfGel2+6888509NFHpy984Qtl+cBYtGhROvjgg1cYTq2vr09vv/12euKJJ9JFF12Utt566/S1r30tnXrqqcXFJcpxf5q7jz7pDxs2LG288cbFqVtHHnlkWn/99dNjjz1WvNzChQvTN77xjUbvvNG0LL/f7r777tSlS5fUvXv31LZt2zRo0KA0a9asNGPGjGKRWv6u8vIFbfh8jRs3Lt14440ppZROPPHEdPrpp6dnnnkmrbnmmumBBx5odNmXXnopbbjhho1G/lNKafDgwalfv35lUaJSWva423nnndPmm2+ezjnnnDR27Nh06qmnpi233DI9+OCDxcsNHTo07bDDDk2+uN9yyy3p+eefL/78yiuvpEsuuSSdd955xfOmTJmSdthhh7T//vsXv4evnAwZMiRtsMEG6fTTT08HHXRQ6tq1a3GRhVdeeSX96Ec/St27d0+FQiHtt99+Tfq1YcmSJenggw9OhUIhtW/fPg0fPjw9/fTTafjw4emwww5Lb731Vlq4cGH6xz/+kR555JFiYWzKMzHK9fOS/83HHSe/99576e9//3u65JJL0p///Ocm8Z2pStQq8NF3+fv27dto9aF58+all156Kd1zzz1pypQp6aqrrirbEZvFixenr371q8XVBFNK6S9/+Us67bTTUvv27VOPHj3S9ttvnx544IH07W9/O33jG99Ip556aqMXGT5/5557blp33XXT5MmTi18+19DQkA455JC0/vrrpxNOOCGdeuqpaZdddkk9evTwzlsT98ADD6S11lorXX311amuri7df//9qVAopEMPPTTNnDkzzZgxI2233XZps802W2E0uSlrSgc6tbW1aY899kj9+vVL3/jGN1JlZWWaOnVqqqmpSd/85jfTgQcemB5//PHi5d9///209dZbp3vuuSel1Hhfy+VzN0899VSqqqpK5557bjr11FPTDjvskAYOHJguu+yydPrpp6c11lgj7bjjjukrX/lKs3hjbebMmekrX/lK8atJ5syZkzbeeOPUtm3b9IMf/KDRZf/+97+nnj17poMOOmiFgpynsWPHpq5du6a///3vKaWUbrvtttSmTZviUtLL/eMf/0i33HJL8XHXlF8bxo0bl4488sj061//OvXv3z+deOKJ6ZBDDkldunRJV1555QqXb8ojb9OnT0+nnXZa+sc//pF3lP/pvx0nz5kzJ7300kvp5ptvzivep6ZErUJHHXVU2n///YsHqX/961/T/vvvn7p165Z22WWXVF9fX9YHAjU1NWnLLbdMxx9/fHrhhRfShRdemLp165a+9a1vpVGjRqVrrrkmbbnllulHP/pR8TpN+cm2OZg9e3bafffdi+94v/nmm2n8+PHphBNOSLfffns6+uij0xFHHJEGDBiQTjvttOLjr5wfh3y8mpqadMIJJxQ/T/naa6+lzTbbLB100EGpqqoqffOb30zTp09P06dPTzvttFN67bXXck68agwdOnSFhVLK3ezZs1O3bt1SoVAofqVASindc889adddd039+vVLl19+ebrnnnvSHnvskXr27NnogK6cVsp85ZVX0nnnnZfOP//84nn33ntv2n333dPBBx+c7rnnnjRhwoR01llnpYsvvji99NJLOaZddRYuXJhSWvY54Tlz5qRJkyalL3zhC+krX/lKeuaZZxpddsqUKalr167p8MMPL053zNu1116bdtlll5RSSrfffnvq0KFDuuqqq1JKyxZOmDBhwgrXaYqvDZdddllxsYylS5emo48+Oh1zzDGpvr4+XX/99em4445LhUIhFQqFZrVC5D//+c/UtWvXdNJJJzWZrw74uOPkLbfcMvXr1y/V1tY2qeNKJWoVmTBhQurUqVN68cUX06233pqOOeaY1K5du3TqqacW311sCv7617+mVq1apU022SR16NAh/eY3vyl+p1V9fX3ac889Gw3DNqUHe3M0Z86ctNFGG6Wzzz47TZw4MR166KHpy1/+ctphhx3SxhtvXFzd66MvjE3xRZJl6urq0m233ZZeeeWVNHv27LT99tunY489NqW0bDnfQqGQ9t577/Tmm282m/v5jDPOSL169SqbEZlPau7cuWmfffZJu+yyS9pjjz3SH/7wh+K2sWPHpu9///upqqoq9e7dOw0YMKBsV8msqalJvXr1Suuvv34aOnRoo2333ntv2nXXXdOBBx64QqloLmpqatI222yTDjvssDR79uw0adKkVF1dnQYNGpT++c9/NrrsU089VVZvXFx//fXp8MMPT/fff39q3759sUCltGxq5g9/+MP07rvv5pjws6uvr0/nn39+atmyZRo4cGB66KGH0pIlS1LPnj3Tz372s+JlTj/99LTnnnuW3d/XZ/XMM8+knj17puOOO67si1RzOU7+KCVqFRkxYkRae+21U69evVLnzp3Tj3/84/Too482ukxTKRwzZsxITz755AoHLUuXLk0HH3xwcSSqqexPc3fNNdekjh07psrKyvTDH/6w+CHvww8/vNHSySm5z5qD5d+tccMNN6SddtqpuIjEzTffnPr375822WSTJjGX/JN44IEH0q677lo2nwn6NN5+++20zz77pF133bVRkUpp2apUc+fOLfvv4nn66afTFltskXbeeecVDtTuu+++tN122xVHYJrjc8yUKVNSr1690jHHHJPmzJmTHnvssWKRKueRjeeffz61bt06FQqFdN111xXPX7hwYdprr73Sscce22zur+eeey4dcMAB6ctf/nI6+uij04033pgOOuig9NRTTxUvs7JV+JqDp59+OvXs2TMdf/zxxeXby1FzOk5eTolaBRYvXpyOO+64tPPOO6ezzjqr0YtiU3tAfJy6urr0ox/9KG200UbNZqpGc/LGG280ul+WLl2adtttt3T22WfnmIpSOvfcc9PWW29d/PD+0KFD0xVXXNEkP2j8cebPn1+cUtWUvfbaa+nrX/962mOPPdK1116blixZknbZZZfil5enVF7T91bmH//4R9puu+3SCSecsEKReuCBB9L06dNzSvb5ePrpp9N2223XqEhtuumm6Vvf+lZZH7jefvvtqW3btumHP/xhGj9+fBo3blzaY4890pe+9KVm8Rmoj/r3v/+d7rrrrtSrV6/UunXrtM466zRaBCSl5rOv/+npp59OvXv3TgcccEBZ/i021+PkQkopBZ9ZTU1NpJSiqqoqCoVCNDQ0RIsWLfKOtUrceOONMWXKlLj11ltj7Nixsf322+cdqayklKJQKOQdIyIiFixYEFOnTo2LL7443njjjXj66aejVatWeccqS+V0v30azzzzTOy0007Rq1evaNOmTUyZMiUeffTR+NKXvpR3NFbi9ddfj8GDB8fzzz8fdXV10a5du3jqqaeidevWeUf7xJ555pk47rjjomfPnnH66afHVlttlXekz9UzzzwTxxxzTPTs2TMuvfTSmDp1avzgBz+IBx54IDbaaKO8463U0qVL47bbboshQ4ZERMSGG24YG220Udx5552xxhprxNKlS6Nly5Y5p1z1fvSjH8Vll10WO+64Y4wfPz7vOJ+Lv//973HjjTfGFVdcEXPmzIm1114770iNNMfjZCWqBJr6wdlHvfjii3HSSSdFx44d44ILLoju3bvnHansvPvuu7HBBhvkHSNSSjFx4sS49NJLY/HixfHnP/+5Wb9Iflblcr99FpMmTYpf//rXUVVVFSeffHL06NEj70j8F2+//XY89dRT8e6778ZRRx0VrVq1iiVLljSpNzqeeeaZOOmkk2LTTTeNn/zkJ7HlllvmHelz9cwzz8QJJ5wQm266afz2t7+N1q1bR9u2bfOO9T/9+9//jnnz5kVFRUVUV1dHoVBoco+9T+Kjx1+TJ0+OHXbYIVq2bNmsjsv+l0svvTSeeOKJuPDCC2PzzTfPO85KNZf7Q4nif3rvvfeioqIiqqqq8o5Sdt54443YbLPNYvTo0fGd73wn7zhRV1cX//rXv2LbbbeNFi1aNMsXyVWh3O63z6KhoSEKhUKzeEFa3TTVNzimTJkSQ4YMiZtvvjk6deqUd5zP3ZQpU2Lw4MFxyy23NNn9bw6jAB/nPw/Qm+rf2ac1YcKEOP744+ORRx5pso/PpkKJgs9g/vz5cfrpp0f79u1j1KhRecdppDm/SH5W5Xy/QVPw4YcfRps2bfKOkZvVff8pb4sWLWoSI6RNnSMs+Aw6dOgQp512Wrz44ouxePHivOM0okB9vHK+36ApWN0LxOq+/5Q3BerzYSQKVoGFCxdGu3bt8o5BRu43AODTUKIAAAAyMN8HAAAgAyUKAAAgAyUqZ3V1dTFixIioq6vLO8oqZ9+aJvvWNNm3psm+NU32rWmyb01Tue6bz0TlrLa2NqqqqqKmpiYqKyvzjrNK2bemyb41TfatabJvTZN9a5rsW9NUrvtmJAoAACADJQoAACCDVnkHKEcNDQ0xa9as6NChQxQKhZL+rtra2kb/bU7sW9Nk35om+9Y02bemyb41Tfatafq89y2lFPPnz4+NNtooWrT4+PEmn4laiTfffDOqq6vzjgEAAORg5syZ0blz54/dbiRqJTp06JB3hJKqqanJO0LJdOy4dt4RSqZFi5Z5RyiZJUvq844AAFD0v/qAErUSpZ7Cl7dyWtlkVWvO911z3jcAgHLyv467LCwBAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQQclL1Ny5c2PBggUl/R0ffvhh/Pvf/y7p7wAAAIgoUYlasmRJ3HfffXHwwQdHp06d4tVXX436+vr4/ve/H506dYo2bdrEJptsEiNHjixeZ8aMGbHffvtF+/bto7KyMg455JB49913i9v/8Y9/xK677hodOnSIysrK2GGHHeLJJ5+MiIh33303Nt5449h///3j7rvvjsWLF5ditwAAAFZtiXr22WfjzDPPjM6dO8eRRx4Z6623XowfPz623Xbb+OUvfxn33ntv3HbbbfHiiy/GTTfdFF26dImIiIaGhthvv/1izpw5MXHixHjooYfitddei0MPPbR424cffnh07tw5pkyZEk899VQMHTo01lhjjYiI2GSTTWLSpEmxySabxIknnhidOnWKU045JZ566qlPlLuuri5qa2sbnQAAAFamkFJKn+UGZs+eHTfeeGNcf/31MW3atNhnn33iiCOOiH333Tdat25dvNwpp5wS06ZNi4cffjgKhUKj23jooYdi7733jtdffz2qq6sjIuJf//pX9OjRIyZPnhy9e/eOysrKuOKKK+Koo476r3mWLFkSY8eOjT/84Q/x5z//OTbffPM46qij4ogjjogNNthgpdcZMWJE/PSnP/0s/wxNyme8y8taq1Zr5B2hZFq0aJl3hJJZvLgu7wgAAEU1NTVRWVn5sds/80jUFVdcEaeddlq0b98+Xnnllbj77rvjwAMPbFSgIiIGDRoUU6dOjW7dusUpp5wSDz74YHHb888/H9XV1cUCFRGx1VZbxVprrRXPP/98REScccYZcdxxx8Xuu+8eF110Ubz66qsrzdOqVav4xje+Ebfffnu8/vrrseGGG8aQIUMaTR38T8OGDYuampriaebMmZ/lnwQAAGjGPnOJOuGEE+K8886Ld955J3r06BFHH310jBs3LhoaGhpdrmfPnvH666/HeeedF4sWLYpDDjkkDjrooE/8e0aMGBHTpk2Lr3/96zFu3LjYaqut4u67717hcimleOSRR+L444+P7t27xyuvvBLnnHNOnHHGGR972xUVFVFZWdnoBAAAsDKfeTrfR/3tb3+L66+/Pm699dbo0KFDHH744XHEEUdEjx49VrjsAw88EAMGDIjZs2fHU0899bHT+aZMmRK9evVa4fqHHXZYfPDBB3HvvfdGRMRLL70UN9xwQ9x4443x/vvvx0EHHRRHHXVU9OvXb4Xpg/9LbW1tVFVVfYp/gabBdL6myXQ+AIDPx/+azrdKS9RyH374YfzpT3+K0aNHx8MPPxzPPPNMPPTQQ9GpU6fYfvvto0WLFvGzn/0s7rvvvnjrrbeiUChEz549o0OHDjFq1KhYsmRJfPe734327dvHhAkTYtGiRTFkyJA46KCDomvXrvHmm2/GUUcdFd/61rfi4osvjhkzZkTXrl2jf//+xfPXXHPNT51fiWq6lKimSYkCAMrJ/ypRrUrxS9u0aRMDBw6MgQMHxqxZs6J9+/bRoUOH+NnPfhYvv/xytGzZMnr37h33339/tGixbEbhPffcEz/4wQ9il112iRYtWsSAAQPiiiuuiIiIli1bxuzZs+PII4+Md999N9Zdd9048MADi4tBrLvuuvH666/HF77whVLsDgAAQFFJRqKaOiNRTZeRqKbJSBQAUE5KvjofAADA6kSJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyECJAgAAyKBV3gHKWyEKhULeIVa5TTfdNu8IJfOlbfrnHaFk+n3963lHKJmrLzsn7wgl07Jl832aXbiwNu8IfAotWrTMO0LJLFlSn3eEEmp+xyP/J+UdADIzEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJBBq7wD/KeJEyfGiSeeGG3atGl0fkNDQ/Tr1y8mT54cdXV1K1xvwYIFMW3atBg1alTccMMN0apV412rr6+Ps88+Ow4//PCS5gcAAJq3sitRixYtioEDB8aIESManT99+vQYOnRoFAqFmDp16grX69+/f6SUYu7cuXHllVdG//79G20fPXp0zJ8/v3TBAQCA1YLpfAAAABmU3UhUHurq6hpNEaytrc0xDQAAUM6MREXEyJEjo6qqqniqrq7OOxIAAFCmlKiIGDZsWNTU1BRPM2fOzDsSAABQpkzni4iKioqoqKjIOwYAANAEGIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIoOxW56uqqooxY8bEmDFjVti21157xbx586JXr14rvW6LFi2ic+fOMXjw4JVuHz58+CrNCgAArH4KKaWUd4hyU1tbG1VVVRFRiEKhkHecVa5Ll63zjlAya1Wtn3eEkun39a/nHaFkrr7snLwjlEzLlmX3XtUqs3Bhbd4R+BRatGiZd4SSWbKkPu8IJdT8jkf+j0NRyk9NTU1UVlZ+7HbT+QAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJolXeActayZasoFAp5x1jlZs16Je8IJbPddrvlHaFk/jXl2bwjlMxaa62fd4SSWbRoQd4RSqZly+b7EtKi0HzfY0yR8o5QQs3vNRvKV3P9e/tkz5HN91UCAACgBJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADFqtyhubOHFinHjiidGmTZtG5zc0NES/fv1i8uTJUVdXt8L1FixYENOmTYtRo0bFDTfcEK1aNY5VX18fZ599dvTp0yf23nvvaNeu3Qq30bVr17j77rvjgAMOiNdff32F7QsXLoyxY8fGZptt9hn3EgAAWJ2t0hK1aNGiGDhwYIwYMaLR+dOnT4+hQ4dGoVCIqVOnrnC9/v37R0op5s6dG1deeWX079+/0fbRo0fH/PnzY/HixdG3b98YPXr0CrfRp0+fiIh4++23V/o7Bg0aFIsXL/6UewYAALCM6XwAAAAZrNKRqKaqrq6u0TTD2traHNMAAADlzEhURIwcOTKqqqqKp+rq6rwjAQAAZUqJiohhw4ZFTU1N8TRz5sy8IwEAAGXKdL6IqKioiIqKirxjAAAATYCRKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAxW6ep8VVVVMWbMmBgzZswK2/baa6+YN29e9OrVa6XXbdGiRXTu3DkGDx680u3Dhw+Ptm3bxnPPPbfS29hmm20iIqJ79+4f+zvatm37SXcFAABgpQoppZR3iHJTW1sbVVVV0bLlGlEoFPKOs8q1bNl8V7bfZ58T845QMh98UJN3hJJ59tmJeUcomUWLFuQdoWQ++GBe3hFKpkWh+U7USNF8X/br6+vyjsCn0nwfk81b8ztGXmbZ47GmpiYqKys/9lLN91UCAACgBJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADJQoAACADFrlHYDPX339h3lHKJk333wx7wgls9nmX8o7QslUVLTLO0LJtGrVOu8IJfPhhx/kHaFkmvO+tWvXIe8IJdOcX9+g/KS8A+TKSBQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGShQAAEAGrfIOUA7q6uqirq6u+HNtbW2OaQAAgHJmJCoiRo4cGVVVVcVTdXV13pEAAIAypURFxLBhw6KmpqZ4mjlzZt6RAACAMmU6X0RUVFRERUVF3jEAAIAmwEgUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABq3yDlDOUmqIiELeMcjgxRf+nneEknnxxcl5RyiZysp1845QMhtu2DXvCCUzb967eUcomUKh+T737/vNk/KOUDK33XJJ3hFKKOUdABopFJrnWExKKT7J31vz3HsAAIASUaIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyaJV3gHJQV1cXdXV1xZ9ra2tzTAMAAJQzI1ERMXLkyKiqqiqeqqur844EAACUKSUqIoYNGxY1NTXF08yZM/OOBAAAlCnT+SKioqIiKioq8o4BAAA0AUaiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMmiVd4By1tDQEIVCIe8Yq1xKDXlHKJna+bPzjsCnsGjR/LwjlMyA/b6Td4SS6bThpnlHKJkx912Vd4SSmfr0+LwjwH9ofsdaq4OUUt4RSuST7ZeRKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAxa5R2gHNTV1UVdXV3x59ra2hzTAAAA5azZj0TddNNN0b59++Lp0UcfXeEyI0eOjKqqquKpuro6h6QAAEBT0OxHor75zW/GjjvuWPx54403XuEyw4YNizPOOKP4c21trSIFAACsVLMvUR06dIgOHTr818tUVFRERUXF55QIAABoypr9dD4AAIBVSYkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADIoFXeAcpZixYtolAo5B1jlVu6tCHvCCWzxhoVeUcomeb4WFyuc+dueUcomdeefyHvCCUzb957eUcomfbt18o7Qsm0aNEy7wgl06rVGnlHKJklSxbnHaGEUt4B+BRatmyeNSKlFA0NS//n5YxEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZNAq7wDloK6uLurq6oo/19bW5pgGAAAoZ0aiImLkyJFRVVVVPFVXV+cdCQAAKFNKVEQMGzYsampqiqeZM2fmHQkAAChTpvNFREVFRVRUVOQdAwAAaAKMRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGSgRAEAAGTQKu8A5SyllHcEMlq8uC7vCCVUyDtAybz11st5RyiZ3b5+SN4RSubRh17JO0LJNDQszTtCyXTosHbeEUqmUGi+z5PNWaHQfN/Tb87Hkg0NDXlHKIlPep8130ctAABACShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGShRAAAAGbTKcuGJEyfGiSeeGG3atGl0fkNDQ/Tr1y8mT54cdXV1K1xvwYIFMW3atBg1alTccMMN0apV419bX18fZ599dvTp0yf23nvvaNeu3Qq30bVr17j77rvjgAMOiNdff32F7QsXLoyxY8fGE088ERdccEG0bt260fYlS5bEEUccEWeddVaWXQYAAGgkU4latGhRDBw4MEaMGNHo/OnTp8fQoUOjUCjE1KlTV7he//79I6UUc+fOjSuvvDL69+/faPvo0aNj/vz5sXjx4ujbt2+MHj16hdvo06dPRES8/fbbK/0dgwYNisWLF8f8+fPjhz/8YQwaNKjR9gkTJsRf/vKXDHsLAACwItP5AAAAMsg0EtVc1dXVNZqGWFtbm2MaAACgnBmJioiRI0dGVVVV8VRdXZ13JAAAoEwpURExbNiwqKmpKZ5mzpyZdyQAAKBMmc4XERUVFVFRUZF3DAAAoAkwEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJBBptX5qqqqYsyYMTFmzJgVtu21114xb9686NWr10qv26JFi+jcuXMMHjx4pduHDx8ebdu2jeeee26lt7HNNttERET37t0/9ne0bds21l9//bjwwgvjyiuvXGH7oEGDPm7XAAAAPpFCSinlHaLc1NbWRlVVVRQKLaJQKOQdZ5VraFiadwQ+leb3WFyuoqJt3hFK5sgTzs47Qsk8+tCf845QMjNnPp93hJLZaqud845QMlOn/jXvCCWzeHF93hFKpjkeay3XnA+zm+v9tuw+S1FTUxOVlZUfeznT+QAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJolXeActayZasoFAp5x1jlGhqW5h2BTyXlHaBk6us/zDtCycx7b17eEUqmS5dt8o5QMm+99VLeEUrmxRf+nneEkllzzbXyjlAyCxfW5B2hZJYsWZx3hJJJqSHvCCXTtm2HvCOUREopFi2a/z8vZyQKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgAyUKAAAgg1Z5B/hPEydOjBNPPDHatGnT6PyGhobo169fTJ48Oerq6la43oIFC2LatGkxatSouOGGG6JVq8a7Vl9fH2effXYcfvjhJc0PAAA0b2VXohYtWhQDBw6MESNGNDp/+vTpMXTo0CgUCjF16tQVrte/f/9IKcXcuXPjyiuvjP79+zfaPnr06Jg/f37pggMAAKsF0/kAAAAyKLuRqDzU1dU1miJYW1ubYxoAAKCcGYmKiJEjR0ZVVVXxVF1dnXckAACgTClRETFs2LCoqakpnmbOnJl3JAAAoEyZzhcRFRUVUVFRkXcMAACgCTASBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkEHZrc5XVVUVY8aMiTFjxqywba+99op58+ZFr169VnrdFi1aROfOnWPw4MEr3T58+PBVmhUAAFj9FFJKKe8Q5aa2tjaqqqqiVavWUSgU8o6zyi1eXJd3BGikUGi+g+IHHXJm3hFKZn7NvLwjlMzjj9+Zd4SSKUTze11brkXLsntveJVZuLAm7wgls2TJ4rwjlExDw9K8I5RMu3aVeUcoiZRSLFo0P2pqaqKy8uP3sfkeuQAAAJSAEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJCBEgUAAJBBIaWU8g5Rbmpra6Oqqur//1TINQtZeThTXgoF71U1Ra3XqMg7QsnUL67LO0LJbLhh17wjlMze+x2Zd4SSmfvuvLwjlMy9916Rd4SSadWqdd4RSiKlFPX1i6KmpiYqKys/9nJe3QEAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJQogAAADJolXeA/zRx4sQ48cQTo02bNo3Ob2hoiH79+sXkyZOjrq5uhestWLAgpk2bFqNGjYobbrghWrVqvGv19fVx9tlnx+GHH17S/AAAQPNWdiVq0aJFMXDgwBgxYkSj86dPnx5Dhw6NQqEQU6dOXeF6/fv3j5RSzJ07N6688sro379/o+2jR4+O+fPnly44AACwWjCdDwAAIIOyG4nKQ11dXaMpgrW1tTmmAQAAypmRqIgYOXJkVFVVFU/V1dV5RwIAAMqUEhURw4YNi5qamuJp5syZeUcCAADKlOl8EVFRUREVFRV5xwAAAJoAI1EAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZlN3qfFVVVTFmzJgYM2bMCtv22muvmDdvXvTq1Wul123RokV07tw5Bg8evNLtw4cPX6VZAQCA1U8hpZTyDlFuamtro6qq6v//VMg1C1l5OFNeCgUD/k1R6zWa79de1C+uyztCyWy4Yde8I5TM3vsdmXeEkpn77ry8I5TMvfdekXeEkmnVqnXeEUoipRT19YuipqYmKisrP/ZyXt0BAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyUKIAAAAyaJV3AACapkKhkHeEktl2u6/lHaFknn76obwjlEy7th3yjlAykx/7a94RSqZz5255RyiZNdZok3eEklmypD7vCCWRUvpElzMSBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkIESBQAAkEGrz/OXTZw4MU488cRo06ZNo/MbGhqiX79+MXny5Kirq1vhegsWLIhp06bFqFGj4oYbbohWrRrHrq+vj7PPPjv69OkTe++9d7Rr126F2+jatWvcfffdq3aHAACA1c7nWqIWLVoUAwcOjBEjRjQ6f/r06TF06NAoFAoxderUFa7Xv3//SCnF3Llz48orr4z+/fs32j569OiYP39+LF68OPr27RujR49e4Tb69Omz6nYEAABYbZnOBwAAkMHnOhJVrurq6hpNI6ytrc0xDQAAUM6MREXEyJEjo6qqqniqrq7OOxIAAFCmlKiIGDZsWNTU1BRPM2fOzDsSAABQpkzni4iKioqoqKjIOwYAANAEGIkCAADIQIkCAADIQIkCAADIQIkCAADIQIkCAADI4HNdna+qqirGjBkTY8aMWWHbXnvtFfPmzYtevXqt9LotWrSIzp07x+DBg1e6ffjw4dG2bdt47rnnVnob22yzzWcLDwAAEBGFlFLKO0S5qa2tjaqqqv//UyHXLGTl4Ux5KRSa74B/odB8nx979RqQd4SSefrph/KOUDKbfGGrvCOUTNt2lXlHKJnOnbvlHaFkJky4Oe8IJbNkSX3eEUoipRRLly6OmpqaqKz8+L+75vvqDgAAUAJKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAZKFAAAQAaFlFLKO0S5qa2tjaqqqrxjAJCTiop2eUcombq6hXlHKJmNN9487wgls802/fOOUDJz576dd4SSac7PJZMm3ZN3hJJIKcWSJfVRU1MTlZWVH3s5I1EAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZKFEAAAAZrBYlqn///nHaaaflHQMAAGgGVosSBQAAsKooUQAAABm0yjtAOairq4u6urriz7W1tTmmAQAAypmRqIgYOXJkVFVVFU/V1dV5RwIAAMqUEhURw4YNi5qamuJp5syZeUcCAADKlOl8EVFRUREVFRV5xwAAAJqA1aJETZgwIe8IAABAM7FaTOfbbbfdYuTIkXnHAAAAmoHVokS9+uqr8e677+YdAwAAaAZWi+l806dPzzsCAADQTKwWI1EAAACrihIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQgRIFAACQQau8AwDQVBXyDlAyKTXkHYFPYfbst/OOUDJPP/1g3hFKZvHiurwjlMz1f70v7wgls98Od+QdoSRSSp/ockaiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMihZiZo7d24sWLCgVDffyIwZMz6X3wMAALBKS9SSJUvivvvui4MPPjg6deoUr776akREzJw5Mw455JBYa621Yu2114799tsvpk+fXrxeQ0NDnHvuudG5c+eoqKiI7bbbLv7yl78Ut9fX18f3v//96NSpU7Rp0yY22WSTGDlyZHH7UUcdFVtvvXVccskl8fbbb6/KXQIAAGhklZSoZ599Ns4888zo3LlzHHnkkbHeeuvF+PHjY9ttt43FixfHXnvtFR06dIhHH300Hn/88Wjfvn0MGDAg6uvrIyLi8ssvj0svvTR+/vOfxz//+c/Ya6+94pvf/Ga8/PLLERHxy1/+Mu6999647bbb4sUXX4ybbropunTpUvz9t912W5xwwglx6623RnV1deyzzz5x6623xocffviJ8tfV1UVtbW2jEwAAwMoUUkrp01xx9uzZceONN8b1118f06ZNi3322SeOOOKI2HfffaN169bFy914441x/vnnx/PPPx+FQiEilo0srbXWWvGnP/0p9txzz9h4443je9/7XgwfPrx4vS9/+cvRu3fv+NWvfhWnnHJKTJs2LR5++OHibXyc559/Pq6//vq46aabYsGCBXHooYfGoEGDok+fPh97nREjRsRPf/rTT/PPALAa++/Px01Z69YVeUcomfr6T/YGY1PUpk37vCOUTGXlOnlHKJnFi+vyjlAy1//1vrwjlMx+O/TOO0JJLKtGKWpqaqKysvJjL/epR6KuuOKKOO2006J9+/bxyiuvxN133x0HHnhgowIVEfGPf/wjXnnllejQoUO0b98+2rdvH2uvvXZ8+OGH8eqrr0ZtbW3MmjUrdt5550bX23nnneP555+PiIhBgwbF1KlTo1u3bnHKKafEgw8++LG5unfvHhdddFG88cYbMXTo0Pj9738fAwYM+K/7MmzYsKipqSmeZs6c+Sn/VQAAgOau1ae94gknnBCtWrWKP/zhD9GjR4/41re+FUcccUT0798/WrT4v262YMGC2GGHHeKmm25a4TbWW2+9T/S7evbsGa+//nqMHTs2Hn744TjkkENi9913jzvuuGOFy86cOTNuuummuOGGG+L111+Pgw8+OI4++uj/evsVFRVRUdF833UEAABWnU89ErXRRhvFj370o3jppZfiL3/5S7Ru3ToOPPDA2GSTTWLo0KExbdq0iFhWgF5++eVYf/3144tf/GKjU1VVVVRWVsZGG20Ujz/+eKPbf/zxx2OrrbYq/lxZWRmHHnpo/O53v4tbb7017rzzzpgzZ05ERMyfPz9Gjx4dX/va16JLly5x3333xRlnnBHvvPNO3HTTTbH77rt/2t0EAABoZJUsLNG3b9+4+uqr45133olLLrkkpk6dGttuu208++yzcfjhh8e6664b++23Xzz66KPx+uuvx4QJE+KUU06JN998MyIihgwZEhdffHHceuut8eKLL8bQoUNj6tSpceqpp0ZExGWXXRY333xzvPDCC/HSSy/F7bffHhtuuGGstdZaERGx//77x09/+tP4yle+Ei+99FI8+uijceyxx/7XeYwAAACfxqeezrcybdq0iYEDB8bAgQNj1qxZ0b59+2jXrl088sgjcdZZZ8WBBx4Y8+fPj4033jh22223Ysk55ZRToqamJs4888x47733Yquttop77703Nt9884iI6NChQ/zsZz+Ll19+OVq2bBm9e/eO+++/vzht8Ne//nVsscUW/3PRCQAAgM/qU6/O15zV1tZGVVVV3jEAylzzfePK6nxNk9X5miar8zVNVucDAADgE1OiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMlCiAAAAMmiVdwAAmqqUd4CSaWhoyDsCn0JqWJp3hJJJzfgxuXBhbd4RSmajtTrmHaFkUmq+j8lPwkgUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABqtFierfv3+cdtppeccAAACagdWiRAEAAKwqShQAAEAGrfIOUA7q6uqirq6u+HNtbW2OaQAAgHJmJCoiRo4cGVVVVcVTdXV13pEAAIAypURFxLBhw6KmpqZ4mjlzZt6RAACAMmU6X0RUVFRERUVF3jEAAIAmYLUoURMmTMg7AgAA0EysFtP5dttttxg5cmTeMQAAgGZgtShRr776arz77rt5xwAAAJqB1WI63/Tp0/OOAAAANBOrxUgUAADAqqJEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZKBEAQAAZNAq7wDlKKWUdwQAcuR1oGlqzvdbQ0ND3hFKpjnfbwvmz887Ap/S/3pcFlJzfuR+Sm+++WZUV1fnHQMAAMjBzJkzo3Pnzh+7XYlaiYaGhpg1a1Z06NAhCoVCSX9XbW1tVFdXx8yZM6OysrKkv+vzZt+aJvvWNNm3psm+NU32rWmyb03T571vKaWYP39+bLTRRtGixcd/8sl0vpVo0aLFf22epVBZWdnsHvTL2bemyb41TfatabJvTZN9a5rsW9P0ee5bVVXV/7yMhSUAAAAyUKIAAAAyUKJyVlFRET/5yU+ioqIi7yirnH1rmuxb02Tfmib71jTZt6bJvjVN5bpvFpYAAADIwEgUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABkoUAABABv8PneT/7SLJsu0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
