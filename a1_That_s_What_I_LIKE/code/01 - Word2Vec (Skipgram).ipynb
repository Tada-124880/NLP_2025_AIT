{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.5.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/tadasuttaket/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.categories()\n",
    "\n",
    "#returns a list of sentences, where each sentence is a list of words from the \"news\" category of the Brown Corpus\n",
    "corpus = brown.sents(categories=\"news\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11352"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_word = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = last_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electric'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[769]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    window_size = 4\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 4 words\n",
    "            # Collect outside words from the window (i-2 to i+2), excluding the center word\n",
    "            outside = [word2index[doc[j]] for j in range(i - window_size, i + window_size + 1) if j != i]\n",
    "            #for each of these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                #center, outside1;   center, outside2;   center, outside3;   center, outside4\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8797],\n",
       "       [10999]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape  #batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3288],\n",
       "       [6362]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14395"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs_size = len(vocabs)\n",
    "vocabs_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocabs_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.LongTensor(x)\n",
    "embedding(x_tensor).shape  #(batch_size, 1, emb_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size, word2index):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "\n",
    "        self.word2index = word2index\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embed(self, word):\n",
    "        word2index = self.word2index\n",
    "        \n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except:\n",
    "            index = word2index['<UNK>']\n",
    "            \n",
    "        word = torch.LongTensor([index])\n",
    "        \n",
    "        embed_c = self.embedding_center(word)\n",
    "        embed_o = self.embedding_outside(word)\n",
    "        embed   = (embed_c + embed_o) / 2\n",
    "        \n",
    "        return embed[0][0].item(), embed[0][1].item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14392, 14393, 14394],\n",
       "        [    0,     1,     2,  ..., 14392, 14393, 14394]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(14395, 2)\n",
       "  (embedding_outside): Embedding(14395, 2)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Skipgram(voc_size, 2, word2index)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8228, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model      = Skipgram(voc_size, emb_size, word2index)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    100 | Loss: 9.996923 | time: 0m 38s\n",
      "Epoch    200 | Loss: 10.967303 | time: 1m 17s\n",
      "Epoch    300 | Loss: 10.310737 | time: 1m 56s\n",
      "Epoch    400 | Loss: 10.675185 | time: 2m 34s\n",
      "Epoch    500 | Loss: 11.706662 | time: 3m 12s\n",
      "Epoch    600 | Loss: 9.008938 | time: 3m 50s\n",
      "Epoch    700 | Loss: 10.603891 | time: 4m 28s\n",
      "Epoch    800 | Loss: 9.754954 | time: 5m 6s\n",
      "Epoch    900 | Loss: 8.667796 | time: 5m 43s\n",
      "Epoch   1000 | Loss: 10.018503 | time: 6m 21s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(path_to_file):\n",
    "    # Open the file in read mode\n",
    "    try:\n",
    "        with open(path_to_file, 'r') as file:\n",
    "            content = file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {path_to_file} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_semantic = \"../Test/word-test.v1-capital.txt\"\n",
    "file_path_syntactic = \"../Test/word-test.v1-past-tense.txt\"\n",
    "\n",
    "content_semantic = open_file(file_path_semantic)\n",
    "content_syntactic = open_file(file_path_syntactic)\n",
    "\n",
    "semantic = []\n",
    "syntactic = []\n",
    "\n",
    "for sent in content_semantic:\n",
    "    semantic.append(sent.strip())\n",
    "\n",
    "for sent in content_syntactic:\n",
    "    syntactic.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space = []\n",
    "\n",
    "for word in vocabs:\n",
    "    vector_space.append(model.get_embed(word))\n",
    "\n",
    "vector_space = np.array(vector_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scipy version\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n",
    "    return cos_sim\n",
    "\n",
    "def cos_sim_scores(vector_space, target_vector):\n",
    "    scores = []\n",
    "    for each_vect in vector_space:\n",
    "        each_vect = tuple(each_vect)\n",
    "        target_vector=tuple(target_vector)\n",
    "        scores.append(cos_sim(target_vector, each_vect))\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(model, test_data):\n",
    "    words = test_data.split(\" \")\n",
    "\n",
    "    embed0 = np.array(model.get_embed(words[0]))\n",
    "    embed1 = np.array(model.get_embed(words[1]))\n",
    "    embed2 = np.array(model.get_embed(words[2]))\n",
    "\n",
    "    similar_vector = embed1 - embed0 + embed2\n",
    "\n",
    "    similarity_scores = cos_sim_scores(vector_space, similar_vector)\n",
    "    max_score_idx = np.argmax(similarity_scores)\n",
    "    similar_word = index2word[max_score_idx]\n",
    "\n",
    "    result = False\n",
    "    if similar_word == words[3]:\n",
    "        result = True\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_total = len(semantic)\n",
    "sem_correct = 0\n",
    "for sent in semantic:\n",
    "    if similarity(model, sent):\n",
    "        sem_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "print(sem_correct)\n",
    "print(sem_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "sem_accuracy = sem_correct / sem_total\n",
    "print(f\"Semantic accuracy: {sem_accuracy:2.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactixc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_total = len(syntactic)\n",
    "syn_correct = 0\n",
    "for sent in syntactic:\n",
    "    if similarity(model, sent):\n",
    "        syn_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "syn_accuracy = syn_correct / syn_total\n",
    "print(f\"Syntactic accuracy: {syn_accuracy:2.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Test/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "content = open_file(file_path)\n",
    "\n",
    "sim_data = []\n",
    "\n",
    "for sent in content:\n",
    "    sim_data.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(model, test_data):\n",
    "    words = test_data.split(\"\\t\")\n",
    "\n",
    "    embed0 = np.array(model.get_embed(words[0].strip()))\n",
    "    embed1 = np.array(model.get_embed(words[1].strip()))\n",
    "\n",
    "    similarity_model = embed1 @ embed0.T\n",
    "    similarity_provided = float(words[2].strip())\n",
    "\n",
    "    return similarity_provided, similarity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_scores = []\n",
    "model_scores = []\n",
    "for sent in sim_data:\n",
    "    ds_score, model_score = compute_similarity(model, sent)\n",
    "\n",
    "    ds_scores.append(ds_score)\n",
    "    model_scores.append(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between models’ dot product and the provided similarity metrics is 0.10.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "corr = spearmanr(ds_scores, model_scores)[0]\n",
    "\n",
    "print(f\"Correlation between models’ dot product and the provided similarity metrics is {corr:2.2f}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
