{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchdata\n",
    "# !pip3 install torchtext==0.16.2\n",
    "# !pip3 install torch==2.2.0\n",
    "# !pip3 install datasets\n",
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !pip3 install pythainlp\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.2'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'input_text' #en\n",
    "TRG_LANGUAGE = 'translated_text' #th\n",
    "\n",
    "dataset = datasets.load_dataset(\"kvush/english_thai_texts\", split={'train': 'train[:70%]', 'validation': 'train[70%:90%]', 'test': 'train[90%:100%]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "type(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row['input_text'], row['translated_text']) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 41901 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [(row['input_text'], row['translated_text']) for row in dataset['validation']]\n",
    "test = [(row['input_text'], row['translated_text']) for row in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11972"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize\n",
    "\n",
    "def thtokenizer(sentence):\n",
    "    # Tokenize the sentence using PyThaiNLP's word_tokenize function\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = thtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Service, scallops, all - top notch in every way!\n",
      "Tokenization:  ['Service', ',', 'scallops', ',', 'all', '-', 'top', 'notch', 'in', 'every', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!\n",
      "Tokenization:  ['บริการ', 'หอย', 'เชลล์', 'ทั้งหมด', ' ', '-', ' ', 'โดดเด่น', 'ใน', 'ทุก', 'ด้าน', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", sample[1])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 12, 10, 0, 10]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haircut'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16607"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 43])\n",
      "Thai shape:  torch.Size([64, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "\n",
    "        #General Attention\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16607, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11664, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11664, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251392\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 11664\n",
      "______\n",
      "14239888\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 53s\n",
      "\tTrain Loss: 4.956 | Train PPL: 141.958\n",
      "\t Val. Loss: 3.917 |  Val. PPL:  50.259\n",
      "Epoch: 02 | Time: 0m 53s\n",
      "\tTrain Loss: 3.450 | Train PPL:  31.501\n",
      "\t Val. Loss: 2.980 |  Val. PPL:  19.697\n",
      "Epoch: 03 | Time: 0m 53s\n",
      "\tTrain Loss: 2.665 | Train PPL:  14.368\n",
      "\t Val. Loss: 2.554 |  Val. PPL:  12.862\n",
      "Epoch: 04 | Time: 0m 53s\n",
      "\tTrain Loss: 2.178 | Train PPL:   8.828\n",
      "\t Val. Loss: 2.315 |  Val. PPL:  10.124\n",
      "Epoch: 05 | Time: 0m 53s\n",
      "\tTrain Loss: 1.845 | Train PPL:   6.330\n",
      "\t Val. Loss: 2.194 |  Val. PPL:   8.971\n",
      "Epoch: 06 | Time: 0m 54s\n",
      "\tTrain Loss: 1.599 | Train PPL:   4.946\n",
      "\t Val. Loss: 2.110 |  Val. PPL:   8.250\n",
      "Epoch: 07 | Time: 0m 54s\n",
      "\tTrain Loss: 1.411 | Train PPL:   4.100\n",
      "\t Val. Loss: 2.068 |  Val. PPL:   7.910\n",
      "Epoch: 08 | Time: 0m 53s\n",
      "\tTrain Loss: 1.261 | Train PPL:   3.529\n",
      "\t Val. Loss: 2.049 |  Val. PPL:   7.758\n",
      "Epoch: 09 | Time: 0m 53s\n",
      "\tTrain Loss: 1.138 | Train PPL:   3.120\n",
      "\t Val. Loss: 2.051 |  Val. PPL:   7.778\n",
      "Epoch: 10 | Time: 0m 53s\n",
      "\tTrain Loss: 1.035 | Train PPL:   2.816\n",
      "\t Val. Loss: 2.066 |  Val. PPL:   7.895\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}_general.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/Seq2SeqTransformer_general.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.030 | Test PPL:   7.617 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "meta = {\n",
    "    'token_transform': token_transform,\n",
    "    'vocab_transform': vocab_transform,\n",
    "}\n",
    "meta_name = 'general-attention.pkl'\n",
    "pickle.dump(meta, open('models/general-attention.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service, scallops, all - top notch in every way!'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  153,    6, 2910,    6,   64,   58,  508, 1441,   19,  250,  219,\n",
       "          11,    3])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   28, 1352, 2288,  168,    4,  150,    4,  661,   11,  326,  422,\n",
       "          13,    3])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 11664])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 11664])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11664])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1352, 2288,    4,    4,  150,    4,  168,   11,  326,  422,   13,    3,\n",
       "          13])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หอย\n",
      "เชลล์\n",
      " \n",
      " \n",
      "-\n",
      " \n",
      "ทั้งหมด\n",
      "ใน\n",
      "ทุก\n",
      "ด้าน\n",
      "!\n",
      "<eos>\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 14, 14])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Service',\n",
       " ',',\n",
       " 'scallops',\n",
       " ',',\n",
       " 'all',\n",
       " '-',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'in',\n",
       " 'every',\n",
       " 'way',\n",
       " '!',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'หอย',\n",
       " 'เชลล์',\n",
       " ' ',\n",
       " ' ',\n",
       " '-',\n",
       " ' ',\n",
       " 'ทั้งหมด',\n",
       " 'ใน',\n",
       " 'ทุก',\n",
       " 'ด้าน',\n",
       " '!',\n",
       " '<eos>',\n",
       " '!']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/_hhynlm52_l1mrgsb4gssb1r0000gn/T/ipykernel_38836/59549304.py:17: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "/var/folders/h9/_hhynlm52_l1mrgsb4gssb1r0000gn/T/ipykernel_38836/59549304.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3618 (\\N{THAI CHARACTER YO YAK}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3648 (\\N{THAI CHARACTER SARA E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3594 (\\N{THAI CHARACTER CHO CHANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3660 (\\N{THAI CHARACTER THANTHAKHAT}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3617 (\\N{THAI CHARACTER MO MA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3651 (\\N{THAI CHARACTER SARA AI MAIMUAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3640 (\\N{THAI CHARACTER SARA U}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
