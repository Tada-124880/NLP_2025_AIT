{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchdata\n",
    "# !pip3 install torchtext==0.16.2\n",
    "# !pip3 install torch==2.2.0\n",
    "# !pip3 install datasets\n",
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !pip3 install pythainlp\n",
    "# !pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'input_text' #en\n",
    "TRG_LANGUAGE = 'translated_text' #th\n",
    "\n",
    "dataset = datasets.load_dataset(\"kvush/english_thai_texts\", split={'train': 'train[:70%]', 'validation': 'train[70%:90%]', 'test': 'train[90%:100%]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "type(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row['input_text'], row['translated_text']) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 41901 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [(row['input_text'], row['translated_text']) for row in dataset['validation']]\n",
    "test = [(row['input_text'], row['translated_text']) for row in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11972"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize\n",
    "\n",
    "def thtokenizer(sentence):\n",
    "    # Tokenize the sentence using PyThaiNLP's word_tokenize function\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = thtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Service, scallops, all - top notch in every way!\n",
      "Tokenization:  ['Service', ',', 'scallops', ',', 'all', '-', 'top', 'notch', 'in', 'every', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!\n",
      "Tokenization:  ['บริการ', 'หอย', 'เชลล์', 'ทั้งหมด', ' ', '-', ' ', 'โดดเด่น', 'ใน', 'ทุก', 'ด้าน', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", sample[1])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 12, 10, 0, 10]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haircut'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16607"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 43])\n",
      "Thai shape:  torch.Size([64, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim  = hid_dim            # Hidden dimension\n",
    "        self.n_heads  = n_heads            # Number of attention heads\n",
    "        self.head_dim = hid_dim // n_heads # Dimension of each attention head\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        # Implement Multiplicative attention\n",
    "        self.W = nn.Linear(self.head_dim, self.head_dim)\n",
    "\n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Project Q(Query), K(key), and V(Value) to hidden dimension\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        # Reshape each of them for Multi-Head Attention\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q,K,V = [batch_size, n heads, src len, head_dim]\n",
    "\n",
    "        # Scaled Multiplicative Attention\n",
    "        K_ = self.W(K)\n",
    "        # Q @ W @ KT (W is weight with d2 x d1 dimension [head_dim, head_dim])\n",
    "        energy = torch.matmul(Q, K_.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16607, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11664, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11664, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251392\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 11664\n",
      "______\n",
      "14249392\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 0s\n",
      "\tTrain Loss: 4.966 | Train PPL: 143.454\n",
      "\t Val. Loss: 3.921 |  Val. PPL:  50.457\n",
      "Epoch: 02 | Time: 1m 1s\n",
      "\tTrain Loss: 3.470 | Train PPL:  32.142\n",
      "\t Val. Loss: 3.000 |  Val. PPL:  20.080\n",
      "Epoch: 03 | Time: 0m 57s\n",
      "\tTrain Loss: 2.680 | Train PPL:  14.582\n",
      "\t Val. Loss: 2.557 |  Val. PPL:  12.899\n",
      "Epoch: 04 | Time: 0m 56s\n",
      "\tTrain Loss: 2.180 | Train PPL:   8.848\n",
      "\t Val. Loss: 2.312 |  Val. PPL:  10.093\n",
      "Epoch: 05 | Time: 0m 57s\n",
      "\tTrain Loss: 1.844 | Train PPL:   6.319\n",
      "\t Val. Loss: 2.180 |  Val. PPL:   8.846\n",
      "Epoch: 06 | Time: 0m 56s\n",
      "\tTrain Loss: 1.597 | Train PPL:   4.938\n",
      "\t Val. Loss: 2.106 |  Val. PPL:   8.218\n",
      "Epoch: 07 | Time: 1m 2s\n",
      "\tTrain Loss: 1.410 | Train PPL:   4.095\n",
      "\t Val. Loss: 2.093 |  Val. PPL:   8.110\n",
      "Epoch: 08 | Time: 1m 8s\n",
      "\tTrain Loss: 1.260 | Train PPL:   3.526\n",
      "\t Val. Loss: 2.071 |  Val. PPL:   7.931\n",
      "Epoch: 09 | Time: 1m 7s\n",
      "\tTrain Loss: 1.138 | Train PPL:   3.120\n",
      "\t Val. Loss: 2.054 |  Val. PPL:   7.801\n",
      "Epoch: 10 | Time: 1m 6s\n",
      "\tTrain Loss: 1.037 | Train PPL:   2.820\n",
      "\t Val. Loss: 2.072 |  Val. PPL:   7.942\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}_multiplicative.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/Seq2SeqTransformer_multiplicative.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.046 | Test PPL:   7.733 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service, scallops, all - top notch in every way!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  153,    6, 2910,    6,   64,   58,  508, 1441,   19,  250,  219,\n",
       "          11,    3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   28, 1352, 2288,  168,    4,  150,    4,  661,   11,  326,  422,\n",
       "          13,    3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 11664])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 11664])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11664])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1352, 2288,  168,    4,  150,    4,  164,   11,  326, 5912,   13,    3,\n",
       "          13])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หอย\n",
      "เชลล์\n",
      "ทั้งหมด\n",
      " \n",
      "-\n",
      " \n",
      "สุดยอด\n",
      "ใน\n",
      "ทุก\n",
      "วิถีทาง\n",
      "!\n",
      "<eos>\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 14, 14])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Service',\n",
       " ',',\n",
       " 'scallops',\n",
       " ',',\n",
       " 'all',\n",
       " '-',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'in',\n",
       " 'every',\n",
       " 'way',\n",
       " '!',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'หอย',\n",
       " 'เชลล์',\n",
       " 'ทั้งหมด',\n",
       " ' ',\n",
       " '-',\n",
       " ' ',\n",
       " 'สุดยอด',\n",
       " 'ใน',\n",
       " 'ทุก',\n",
       " 'วิถีทาง',\n",
       " '!',\n",
       " '<eos>',\n",
       " '!']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/_hhynlm52_l1mrgsb4gssb1r0000gn/T/ipykernel_42574/3893484587.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "/var/folders/h9/_hhynlm52_l1mrgsb4gssb1r0000gn/T/ipykernel_42574/3893484587.py:19: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3618 (\\N{THAI CHARACTER YO YAK}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3648 (\\N{THAI CHARACTER SARA E}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3594 (\\N{THAI CHARACTER CHO CHANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3660 (\\N{THAI CHARACTER THANTHAKHAT}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3617 (\\N{THAI CHARACTER MO MA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3626 (\\N{THAI CHARACTER SO SUA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3640 (\\N{THAI CHARACTER SARA U}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3651 (\\N{THAI CHARACTER SARA AI MAIMUAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3623 (\\N{THAI CHARACTER WO WAEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3636 (\\N{THAI CHARACTER SARA I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3606 (\\N{THAI CHARACTER THO THUNG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3637 (\\N{THAI CHARACTER SARA II}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/tadasuttaket/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
