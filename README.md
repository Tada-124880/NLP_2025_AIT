# Direct Preference Optimization (DPO) Model

## ğŸ“Œ Assignment Overview
This assignment implements **Direct Preference Optimization (DPO)** using a fine-tuned **GPT-2 model**. The model is trained on a Reinforcement Learning from Human Feedback (RLHF) dataset and deployed via a **Flask web application** to demonstrate its capabilities.

---

## âœ… Task 1: Finding a Suitable Dataset
### ğŸ” Selected Dataset: **Anthropic HH-RLHF** ([Hugging Face](https://huggingface.co/datasets/psyche/anthropic-hh-rlhf))

- **Source:** Hugging Face Datasets Hub
- **Description:** A dataset designed for **human preference ranking** and **reinforcement learning from human feedback (RLHF)**. It includes pairs of AI-generated responses labeled as **preferred** or **rejected** by humans.
- **Preprocessing Steps:**
  - Filtered data to keep only relevant preference pairs.
  - Tokenized using the **GPT-2 tokenizer**.
  - Applied padding and truncation (max length = **128** tokens).

---

## âœ… Task 2: Training a Model with DPOTrainer
### ğŸ— Model Training Steps
1ï¸âƒ£ **Used `DPOTrainer` from `trl`** to fine-tune GPT-2 on the selected dataset.
2ï¸âƒ£ **Applied LoRA (Low-Rank Adaptation)** to optimize memory usage and training efficiency.
3ï¸âƒ£ **Optimized Hyperparameters:**
   - **Learning Rate:** `1e-3`
   - **Batch Size:** `8`
   - **Max Length:** `128`
   - **Max Prompt Length:** `128`
   - **Max Target Length:** `128`
   - **Repetition Penalty:** `1.2`
   - **No Repeat N-Gram Size:** `3`

### ğŸ”¬ Training Performance
- The model achieved **stable training loss** with **no overfitting**.
- Evaluated with a **held-out test dataset** to ensure generalization.

---

## âœ… Task 3: Pushing the Model to Hugging Face Hub
### ğŸ“¤ Uploaded Model Repository
ğŸ”— **[KittenCat/dpo-finetuned-GPT2-RLHF-dataset](https://huggingface.co/KittenCat/dpo-finetuned-GPT2-RLHF-dataset)**

### ğŸ“„ Steps to Upload
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("./test")
model = AutoModelForCausalLM.from_pretrained("./test")

hf_username = "KittenCat"
model_name = "dpo-finetuned-GPT2-RLHF-dataset"

model.push_to_hub(f"{hf_username}/{model_name}")
tokenizer.push_to_hub(f"{hf_username}/{model_name}")
```

---

## âœ… Task 4: Web Application Development
### ğŸŒ Flask Web App
A **simple, modern web app** was developed using Flask and Bootstrap to interact with the trained model.

### ğŸš€ Features
- **Minimalist UI** (Bootstrap)
- **Text input and real-time response generation**
- **Mobile-friendly design**

### ğŸ“œ How to Run the Web App
#### **1ï¸âƒ£ Install Dependencies**
```bash
pip install flask transformers torch
```
#### **2ï¸âƒ£ Run the Application**
```bash
python app.py
```
#### **3ï¸âƒ£ Open in Browser**
ğŸ”— **http://127.0.0.1:9999/**

---

## ğŸ† Acknowledgments
- **Dataset:** [psyche/anthropic-hh-rlhf](https://huggingface.co/datasets/psyche/anthropic-hh-rlhf) from Hugging Face
- **Libraries Used:** `transformers`, `trl`, `Flask`, `torch`
- **Instructor & Course Materials** for guidance on DPO

ğŸ’¡ **This project showcases how DPO can fine-tune language models efficiently and make them available through a web app.** ğŸš€

**âš ï¸ Disclaimer:** This project is an academic assignment based on materials from **Chaklam Silpasuwanchai & Todsavad Tangtortan**. Most of the code follows their instructions and coursework.